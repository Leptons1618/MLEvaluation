"""
Comprehensive test suite for enhanced UI components and data preparation features
"""

import sys
import os
from pathlib import Path
import tempfile
import json

# Add src to path
src_dir = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_dir))

import pandas as pd
import numpy as np
import pytest
from components.enhanced_ui_components import (
    safe_mean, safe_median, safe_mode, 
    apply_imputation, make_dataframe_arrow_compatible,
    create_column_type_selector, recommend_imputation_method
)
from utils.data_preparation_enhanced import DataPreparationTools


class TestEnhancedUIComponents:
    """Test suite for enhanced UI components"""
    
    def setup_method(self):
        """Setup test data"""
        self.test_df = pd.DataFrame({
            'numeric_col': [1, 2, 3, np.nan, 5],
            'categorical_col': ['A', 'B', 'A', np.nan, 'C'],
            'mixed_col': [1, 'text', 3.14, np.nan, True],
            'boolean_col': [True, False, True, np.nan, False],
            'target': [0, 1, 0, 1, 0]
        })
    
    def test_safe_calculations_comprehensive(self):
        """Test safe calculation functions with edge cases"""
        
        # Test with normal data
        normal_series = pd.Series([1, 2, 3, 4, 5])
        assert safe_mean(normal_series) == 3.0
        assert safe_median(normal_series) == 3.0
        assert safe_mode(normal_series) == 1.0
        
        # Test with missing values
        missing_series = pd.Series([1, 2, np.nan, 4, 5])
        assert safe_mean(missing_series) == 3.0
        assert safe_median(missing_series) == 3.0
        
        # Test with all missing values
        all_missing_numeric = pd.Series([np.nan, np.nan], dtype='float64')
        assert safe_mean(all_missing_numeric) == 0.0
        assert safe_median(all_missing_numeric) == 0.0
        assert safe_mode(all_missing_numeric) == 0.0
        
        all_missing_categorical = pd.Series([np.nan, np.nan], dtype='object')
        assert safe_mode(all_missing_categorical) == 'Unknown'
        
        # Test with single value
        single_value = pd.Series([42])
        assert safe_mean(single_value) == 42.0
        assert safe_median(single_value) == 42.0
        assert safe_mode(single_value) == 42.0
        
        # Test categorical mode
        categorical_series = pd.Series(['A', 'B', 'A', 'C', 'A'])
        assert safe_mode(categorical_series) == 'A'
        
        print("‚úÖ Safe calculations comprehensive test passed")
    
    def test_arrow_compatibility_comprehensive(self):
        """Test Arrow compatibility with various data types"""
        
        # Test with mixed types
        mixed_df = pd.DataFrame({
            'col1': [1, 'text', 3.14, True, np.nan],
            'col2': [1, 2, 3, 4, 5],
            'col3': ['A', 'B', 'C', 'D', 'E']
        })
        
        safe_df = make_dataframe_arrow_compatible(mixed_df)
          # Mixed column should be converted to string
        assert safe_df['col1'].dtype == 'object'
        assert str(safe_df['col1'].iloc[0]) == '1'
        assert str(safe_df['col1'].iloc[1]) == 'text'
        
        # Check that we have consistent data types
        print(f"col1 dtype: {safe_df['col1'].dtype}")
        print(f"col2 dtype: {safe_df['col2'].dtype}")
        print(f"col3 dtype: {safe_df['col3'].dtype}")
        
        # All columns should be object type after Arrow compatibility conversion
        # (This is the safe approach to avoid Arrow serialization issues)
        assert safe_df['col1'].dtype == 'object'
        assert safe_df['col2'].dtype in ['int64', 'float64', 'object']  # May be converted for safety
        assert safe_df['col3'].dtype == 'object'
        
        print("‚úÖ Arrow compatibility comprehensive test passed")
    
    def test_imputation_methods_comprehensive(self):
        """Test all imputation methods"""
        
        df = self.test_df.copy()
        
        # Test mean imputation
        config_mean = {
            'numeric_col': {'method': 'Mean', 'custom_value': None}
        }
        result_mean = apply_imputation(df, config_mean)
        assert result_mean['numeric_col'].isnull().sum() == 0
        
        # Test median imputation
        config_median = {
            'numeric_col': {'method': 'Median', 'custom_value': None}
        }
        result_median = apply_imputation(df, config_median)
        assert result_median['numeric_col'].isnull().sum() == 0
        
        # Test mode imputation
        config_mode = {
            'categorical_col': {'method': 'Mode', 'custom_value': None}
        }
        result_mode = apply_imputation(df, config_mode)
        assert result_mode['categorical_col'].isnull().sum() == 0
        
        # Test custom value imputation
        config_custom = {
            'categorical_col': {'method': 'Custom Value', 'custom_value': 'MISSING'}
        }
        result_custom = apply_imputation(df, config_custom)
        assert result_custom['categorical_col'].isnull().sum() == 0
        
        # Test drop rows
        config_drop = {
            'numeric_col': {'method': 'Drop Rows', 'custom_value': None}
        }
        result_drop = apply_imputation(df, config_drop)
        assert len(result_drop) < len(df)
        
        print("‚úÖ Imputation methods comprehensive test passed")
    
    def test_column_type_recommendations(self):
        """Test column type recommendations"""
        
        # Test numeric column
        numeric_col = pd.Series([1, 2, 3, 4, 5])
        recommended = recommend_imputation_method(numeric_col, 'numeric_col')
        assert recommended in ['Mean', 'Median']
        
        # Test categorical column
        categorical_col = pd.Series(['A', 'B', 'C', 'A', 'B'])
        recommended = recommend_imputation_method(categorical_col, 'categorical_col')
        assert recommended == 'Mode'
        
        # Test skewed numeric column
        skewed_col = pd.Series([1, 1, 1, 2, 100])  # Highly skewed
        recommended = recommend_imputation_method(skewed_col, 'skewed_col')
        assert recommended == 'Median'  # Should recommend median for skewed data
        
        print("‚úÖ Column type recommendations test passed")
    
    def test_data_preparation_tools(self):
        """Test data preparation tools"""
        
        tools = DataPreparationTools()
        
        # Test with problematic data
        problematic_df = pd.DataFrame({
            'feature1': [1, 2, np.nan, 4, 5],
            'feature2': ['A', 'B', np.nan, 'A', 'C'],
            'high_missing': [np.nan] * 4 + [1],  # 80% missing
            'target': [0, 1, 0, 1, 0]
        })
        
        analysis = tools.analyze_preparation_needs(problematic_df, 'target')
        
        # Should detect missing values
        assert any('missing' in issue.lower() for issue in analysis['issues'])
        
        # Should have suggestions
        assert len(analysis['suggestions']) > 0
        
        # Should have auto-fixes
        assert len(analysis['auto_fixes']) > 0
        
        print("‚úÖ Data preparation tools test passed")
    
    def test_error_handling(self):
        """Test error handling in various scenarios"""
        
        # Test with completely empty DataFrame
        empty_df = pd.DataFrame()
        try:
            safe_df = make_dataframe_arrow_compatible(empty_df)
            assert len(safe_df) == 0
        except Exception as e:
            pytest.fail(f"Empty DataFrame handling failed: {e}")
        
        # Test with invalid imputation config
        invalid_config = {
            'nonexistent_col': {'method': 'Mean', 'custom_value': None}
        }
        try:
            result = apply_imputation(self.test_df, invalid_config)
            # Should not raise exception, should handle gracefully
        except Exception as e:
            pytest.fail(f"Invalid config handling failed: {e}")
        
        # Test with invalid data types
        weird_df = pd.DataFrame({
            'col1': [complex(1, 2), complex(3, 4), complex(5, 6)]
        })
        try:
            safe_df = make_dataframe_arrow_compatible(weird_df)
            # Should convert to string
            assert safe_df['col1'].dtype == 'object'
        except Exception as e:
            pytest.fail(f"Complex number handling failed: {e}")
        
        print("‚úÖ Error handling test passed")
    
    def test_performance_with_large_data(self):
        """Test performance with larger datasets"""
        
        # Create larger test dataset
        large_df = pd.DataFrame({
            'numeric_col': np.random.randn(10000),
            'categorical_col': np.random.choice(['A', 'B', 'C', 'D'], 10000),
            'target': np.random.choice([0, 1], 10000)
        })
        
        # Add some missing values
        missing_indices = np.random.choice(10000, 1000, replace=False)
        large_df.loc[missing_indices, 'numeric_col'] = np.nan
        large_df.loc[missing_indices[:500], 'categorical_col'] = np.nan
        
        # Test safe calculations
        import time
        start_time = time.time()
        
        mean_val = safe_mean(large_df['numeric_col'])
        median_val = safe_median(large_df['numeric_col'])
        mode_val = safe_mode(large_df['categorical_col'])
        
        calc_time = time.time() - start_time
        
        # Should complete in reasonable time (< 1 second)
        assert calc_time < 1.0
        assert not np.isnan(mean_val)
        assert not np.isnan(median_val)
        assert mode_val is not None
        
        # Test Arrow compatibility
        start_time = time.time()
        safe_df = make_dataframe_arrow_compatible(large_df)
        arrow_time = time.time() - start_time
        
        # Should complete in reasonable time (< 2 seconds)
        assert arrow_time < 2.0
        assert len(safe_df) == len(large_df)
        
        print("‚úÖ Performance test passed")
    
    def test_integration_with_streamlit_session_state(self):
        """Test integration with Streamlit session state patterns"""
        
        # Simulate session state dictionary
        session_state = {}
        
        # Test operation logging
        operation_log = []
        
        # Simulate imputation operation
        df = self.test_df.copy()
        config = {
            'numeric_col': {'method': 'Mean', 'custom_value': None}
        }
        
        result_df = apply_imputation(df, config)
        
        # Log the operation
        operation_log.append({
            'operation': 'Missing Value Imputation',
            'details': f"Applied imputation to {len(config)} columns",
            'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),
            'methods': config
        })
        
        session_state['operation_log'] = operation_log
        session_state['modified_df'] = result_df
        session_state['original_df'] = df
        
        # Test session state contents
        assert len(session_state['operation_log']) == 1
        assert 'modified_df' in session_state
        assert 'original_df' in session_state
        
        # Test that modified DataFrame is different from original
        assert not session_state['modified_df'].equals(session_state['original_df'])
        
        print("‚úÖ Streamlit session state integration test passed")


def recommend_imputation_method(series: pd.Series, column_name: str) -> str:
    """Recommend imputation method based on data characteristics"""
    
    if series.dtype in ['int64', 'float64', 'Int64', 'Float64']:
        # For numeric data, recommend median for skewed data, mean for normal
        try:
            skewness = abs(series.skew())
            if skewness > 2:  # Highly skewed
                return 'Median'
            else:
                return 'Mean'
        except:
            return 'Median'
    else:
        # For categorical data, recommend mode
        return 'Mode'


if __name__ == "__main__":
    print("üß™ Running Comprehensive Enhanced UI Components Tests...")
    
    test_suite = TestEnhancedUIComponents()
    
    try:
        test_suite.setup_method()
        test_suite.test_safe_calculations_comprehensive()
        test_suite.test_arrow_compatibility_comprehensive()
        test_suite.test_imputation_methods_comprehensive()
        test_suite.test_column_type_recommendations()
        test_suite.test_data_preparation_tools()
        test_suite.test_error_handling()
        test_suite.test_performance_with_large_data()
        test_suite.test_integration_with_streamlit_session_state()
        
        print("\nüéâ All comprehensive tests passed!")
        print("‚úÖ Arrow serialization fixes working correctly")
        print("‚úÖ Safe calculation functions working correctly")
        print("‚úÖ Imputation methods working correctly")
        print("‚úÖ Error handling working correctly")
        print("‚úÖ Performance is acceptable")
        print("‚úÖ Streamlit integration working correctly")
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
